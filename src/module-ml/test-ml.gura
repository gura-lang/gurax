#!/usr/bin/env gurax
//==============================================================================
// test-ml.gura
//==============================================================================
import(util.tester) {*}
import(ml)

//Number.format@float = '%.3f'
a = @float([[-6, -1, 4], [-5, -1, -5], [-3, 2, 2]])
b = @float([[3, -1, 4], [1, 2, -5], [-2, 4, -4]])
//correct = @float([[5, -2, -5], [5, -4, -2], [-6, 5, -3]])
correct = @float([[1, 0, 0], [0, 0, 0], [0, 0, 0]])

PrintNodeHead(node as ml.Node) = {
	Printf('%s\n', node.typeName)
	Printf('  output     %s\n', node.output)
	Printf('  outputGrad %s\n', node.outputGrad)
}

PrintNodeUnary(node as ml.Node) = {
	Printf('%s\n', node.typeName)
	Printf('  input      %s\n', node.input)
	Printf('  inputGrad  %s\n', node.inputGrad)
	Printf('  output     %s\n', node.output)
	Printf('  outputGrad %s\n', node.outputGrad)
}

PrintNodeBinary(node as ml.Node) = {
	Printf('%s\n', node.typeName)
	Printf('  inputL     %s\n', node.inputL)
	Printf('  inputR     %s\n', node.inputR)
	Printf('  inputGradL %s\n', node.inputGradL)
	Printf('  inputGradR %s\n', node.inputGradR)
	Printf('  output     %s\n', node.output)
	Printf('  outputGrad %s\n', node.outputGrad)
}

TestCase('Gear Node') {
	[
		`(rtn = a |*| ml.ReLU())
		`(rtn = a |*| ml.Sigmoid())
		`(rtn = a |*| ml.Softmax())
		`(rtn = a |*| ml.Tanh())
	].Each {|model|
		t = ml.Trainer(model, nil)
		t.Train(correct)
		PrintNodeUnary(t.node.rtn)
	}
}

TestCase('Unary Node') {
	[
		`(rtn = -a)
	].Each {|model|
		t = ml.Trainer(model, nil)
		t.Train(correct)
		PrintNodeUnary(t.node.rtn)
	}
}

TestCase('Binary Node') {
	[
		`(rtn = a + b)
		`(rtn = a - b)
		`(rtn = a * b)
		`(rtn = a |.| b)
	].Each {|model|
		t = ml.Trainer(model, nil)
		t.Train(correct)
		PrintNodeBinary(t.node.rtn)
	}
}

TestCase('training of ml.Softmax') {
	model = `(x |*| ml.Softmax())
	x = @float([[1, 2, 3]])
	correct = @float([[1, 0, 0]])
	ml.Trainer(model, ml.Optimizer.GradientDescent(0.01)) {|t|
		repeat (1000) {|i|
			printFlag = ((i % 100) == 0)
			printFlag && Printf('%d: %s .. ', i, x)
			t.Train(correct)
			printFlag && Printf('%f\n', t.CalcMeanSquaredError(correct))
		}
	}
}

TestCase('Training test') {
	model = `((a |.| b) |*| ml.Softmax())
	a = @float([[1, 2, 3]])
	b = @float([[3, 2, 1], [3, 1, 2], [4, 1, 2]]) * .1
	correct = @float([[1, 0, 0]])
	ml.Trainer(model, ml.Optimizer.GradientDescent(0.01)) {|t|
		repeat (1000) {|i|
			printFlag = ((i % 100) == 0)
			printFlag && Printf('%d: %s |.| %s = ', i, a, b)
			t.Train(correct)
			printFlag && Printf('%s %f\n', t.result, t.CalcMeanSquaredError(correct))
		}
	}
}
